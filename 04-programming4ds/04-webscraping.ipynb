{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ECON 725: Computer Programming and Data Management in Economics <a class=\"tocSkip\"></center>    \n",
    "# <center>Python - Web scraping <a class=\"tocSkip\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "<hr>\n",
    "\n",
    "- Understand the basics of web scraping\n",
    "- Learn how to use the `MechanicalSoup` library to scrape web pages.\n",
    "- Learn how to scrape data from a website using forms, and pagination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping Basics\n",
    "\n",
    "This lecture is about on how to get \"cheap\" data, off the web and onto our computer. This is a very useful skill to have, as it allows us to collect data that is not available in a downloadable format.\n",
    "\n",
    "Web scraping is the process of extracting information from websites. It simulates the human browsing behavior to extract information from websites. The information is then converted into a structured format that can be further analyzed and stored in a database. There are two ways that web content gets rendered in a browser:\n",
    "\n",
    "1. Server-side rendering: The server sends the HTML content to the browser, which then renders the content.\n",
    "2. Client-side rendering: The server sends a minimal HTML content to the browser, which then executes JavaScript code to render the content.\n",
    "\n",
    "You can read [here](https://www.codeconquest.com/website/client-side-vs-server-side/) for more details (incluiding example scripts), but for the purposed of this lecture, we will focus on the following:\n",
    "\n",
    "### Server-side rendering\n",
    "\n",
    "* The scripts that ‚Äúbuild‚Äù the website are not run on our computer, but rather on a host server that sends down all of the HTML code.\n",
    "E.g. Wikipedia tables are already populated with all of the information ‚Äî numbers, dates, etc. ‚Äî that we see in our browser.\n",
    "* In other words, the information that we see in our browser has already been processed by the host server.\n",
    "* You can think of this information being embeded directly in the webpage‚Äôs HTML.\n",
    "* **Webscraping challenges**: Finding the correct CSS (or Xpath) ‚Äúselectors‚Äù. Iterating through dynamic webpages (e.g. ‚ÄúNext page‚Äù and ‚ÄúShow More‚Äù tabs).\n",
    "* Key concepts: CSS, Xpath, HTML\n",
    "\n",
    "### Client-side rendering\n",
    "* The website contains an empty template of HTML and CSS.\n",
    "E.g. It might contain a ‚Äúskeleton‚Äù table without any values.\n",
    "* However, when we actually visit the page URL, our browser sends a request to the host server.\n",
    "* If everything is okay (e.g. our request is valid), then the server sends a response script, which our browser executes and uses to populate the HTML template with the specific information that we want.\n",
    "* **Webscraping challenges**: Finding the ‚ÄúAPI endpoints‚Äù can be tricky, since these are sometimes hidden from view.\n",
    "* Key concepts: APIs, API endpoints\n",
    "\n",
    "\n",
    "Over this lecture, we'll go over the server-side rendering, which is the most common type of rendering. However, I want to forewarn you that webscraping typically involves a fair bit of detective work. \n",
    "\n",
    "You will often have to adjust your steps according to the type of data you want, and the steps that worked on one website may not work on another. (Or even work on the same website a few months later). \n",
    "\n",
    "**All this is to say that webscraping involves as much art as it does science.**\n",
    "\n",
    "The good news is that both server-side and client-side websites allow for webscraping.1 If you can see it in your browser, you can scrape it!\n",
    "\n",
    "\n",
    "### Caveat: Ethical and legal considerations\n",
    "\n",
    "TLDR; Just because you can scrape it, doesn‚Äôt mean you should.\n",
    "\n",
    "It is currently legal (read more [here](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn)) to scrape public websites, but there are some important caveats to keep in mind: it‚Äôs still important to realise that the tools we‚Äôll be using over these next two lectures are very powerful. A computer can process commands much, much faster than we can ever type them up manually. It‚Äôs pretty easy to write up a function or program that can overwhelm a host server or application through the sheer weight of requests. Or, just as likely, the host server has built-in safeguards that will block you in case of a suspected malicious attack. So, be careful and nice when scraping websites üôÇ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# MechanicalSoup \n",
    "\n",
    "### What is MechanicalSoup?\n",
    "\n",
    "[MechanicalSoup](https://mechanicalsoup.readthedocs.io/en/stable/) is a Python browser automation library built on top of `Requests` (for making HTTP requests) and `BeautifulSoup` (for parsing HTML). It acts as a headless browser, mimicking a browser's behavior without the need for a graphical user interface. This makes it lightweight and efficient compared to full-fledged browser automation tools.\n",
    "\n",
    "Here a simple example of using `MechanicalSoup` to open a webpage, print its title, and close the browser session:\n",
    "```python\n",
    "import mechanicalsoup\n",
    "\n",
    "browser = mechanicalsoup.StatefulBrowser()\n",
    "browser.open(\"https://example.com\")\n",
    "page = browser.get_current_page()\n",
    "print(page.title.text)\n",
    "browser.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use MechanicalSoup for web scraping?\n",
    "\n",
    "`MechanicalSoup` provides some features that make it a valuable tool for web scraping:\n",
    "\n",
    "- Navigation\n",
    "- Form handling\n",
    "- Session management\n",
    "\n",
    "#### Navigation\n",
    "\n",
    "```python\n",
    "# Follow a link by its text\n",
    "browser.follow_link(\"Login\")\n",
    "\n",
    "# Follow a link by its URL\n",
    "browser.visit(\"https://example.com/next_page\")\n",
    "\n",
    "```\n",
    "\n",
    "#### Form handling\n",
    "\n",
    "```python\n",
    "from mechanicalsoup import Browser\n",
    "\n",
    "# Replace with the login URL you want\n",
    "browser = Browser()\n",
    "browser.open(\"https://old.reddit.com\")  # Example login form\n",
    "\n",
    "# Find the login form\n",
    "login_form = browser.select_form('form')  # Replace with specific form identifier if needed\n",
    "\n",
    "# Fill the form fields\n",
    "login_form[\"username\"] = \"your_username\"\n",
    "login_form[\"password\"] = \"your_password\"\n",
    "\n",
    "# Submit the form\n",
    "browser.submit_selected()\n",
    "\n",
    "# Access the response data (assuming successful login)\n",
    "response = browser.soup\n",
    "content = response.find(\"pre\").text\n",
    "print(content)  # This should print the form data submitted\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "#### Session management\n",
    "\n",
    "`MechanicalSoup` automatically manages cookies and other session information:\n",
    "\n",
    "```python\n",
    "# Access data from a protected page after login (session is maintained)\n",
    "content = browser.soup.find(\"div\", class_=\"protected_content\").text\n",
    "print(content)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MechanicalSoup` is a powerful tool for web scraping, but it is not the only one. Other popular libraries include `Scrapy`, `Selenium`, and `BeautifulSoup`. Each has its own strengths and weaknesses, so it's important to choose the right tool for the job.\n",
    "\n",
    "#### MechanicalSoup vs. BeautifulSoup\n",
    "While both libraries are used for web scraping, `MechanicalSoup` and `BeautifulSoup` have distinct functionalities:\n",
    "\n",
    "- `BeautifulSoup`: This library specializes in parsing HTML content. It excels at identifying and extracting data from downloaded HTML code. However, it doesn't handle tasks like form submission, navigation, or session management.\n",
    "- `MechanicalSoup`: Built on top of `BeautifulSoup`, `MechanicalSoup` adds form handling, navigation, and session management functionalities. It allows you to interact with websites more dynamically by mimicking user behavior.\n",
    "If you simply need to extract data from downloaded HTML content, `BeautifulSoup` is sufficient.\n",
    "\n",
    "If your scraping task involves interacting with forms, navigating through pages, or maintaining sessions, `MechanicalSoup` is the better choice.\n",
    "\n",
    "\n",
    "#### MechanicalSoup vs. Selenium\n",
    "\n",
    "Another popular library for web scraping is `Selenium`, as it provides full-fledged browser automation. Here's a breakdown of the key differences between `Selenium` and `MechanicalSoup`:\n",
    "\n",
    "* Functionality\n",
    "\n",
    "    - `Selenium` can handle complex JavaScript, render pages with dynamic content, and automate browser interactions beyond scraping.\n",
    "    - `MechanicalSoup` focuses on scraping tasks and doesn't mimic advanced browser behavior.\n",
    "\n",
    "* Complexity\n",
    "    - `Selenium` has a steeper learning curve due to its comprehensive functionality.\n",
    "    - `MechanicalSoup` offers a simpler API, which makes it easier to learn and use for basic scraping tasks.\n",
    "\n",
    "* Performance\n",
    "    - `Selenium` is slower, and so may require more processing power due to its browser emulation capabilities.\n",
    "    - `MechanicalSoup` is generally faster for simpler tasks as it doesn't involve full browser rendering.\n",
    "\n",
    "\n",
    "If you need to handle advanced JavaScript or mimic complex browser interactions, `Selenium` is the way to go.\n",
    "\n",
    "For most basic scraping tasks, especially for static websites and those with straightforward forms and navigation, `MechanicalSoup` is a more efficient and lightweight option.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with MechanicalSoup\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githack.com/ECON725-EMORY/econ725-fall2024/main/images/mechanical_soup.png\" width=\"400\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "This topic material is based on the [scrapingbee.com](https://www.scrapingbee.com/blog/getting-started-with-mechanicalsoup/) blog and adapted for our purposes in the course.\n",
    "\n",
    "\n",
    "Let's start with a simple example on how to extract the title and the reference links of a webpage using `MechanicalSoup` using this link from [Wikipedia](https://en.wikipedia.org/wiki/Web_scraping). Of course, there's a lot of information that you can scrape from a Wikipedia page, but extracting just the title and the reference links should help you understand how to find HTML tags and how to extract data from deep within the HTML hierarchy of a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanicalsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the website\n",
    "browser = mechanicalsoup.StatefulBrowser()  \n",
    "browser.open(\"https://en.wikipedia.org/wiki/Web_scraping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting the title\n",
    "\n",
    "To extract the title, you first need to understand the structure of the Wikipedia page. Open the page in Google Chrome and press `F12` to open the developer tools. In the `Elements` tab, look for the HTML tag for the title of the page. Here's what it will look like:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://www.scrapingbee.com/blog/getting-started-with-mechanicalsoup/page-title-html_hu929c9a2b4249905d5d24bb430548fc39_226427_1200x0_resize_catmullrom_3.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "To get the title from the page, look for a `span` tag that has the class `mw-page-title-main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Web scraping\n"
     ]
    }
   ],
   "source": [
    "title = browser.page.select_one('span.mw-page-title-main')\n",
    "print(\"title: \" + title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the reference links\n",
    "\n",
    "Next, you'll find and extract all the links from the references section of the web page. First, take a quick peek at the HTML structure of the References section by inspecting it with your browser's developer tools:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://www.scrapingbee.com/blog/getting-started-with-mechanicalsoup/references-structure_hud0b332475e777514f36963071ecd4054_403711_1500x0_resize_catmullrom_3.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "First, select the ordered list (`ol`) tag that has the class references. With that tag selected, you can consider using the method `find_all` method to extract all links inside this `ol` tag. However, on a closer look, you'll find that this section also contains backlinks to places in the same Wikipedia page where each reference was cited:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://www.scrapingbee.com/blog/getting-started-with-mechanicalsoup/backlinks_hud0b332475e777514f36963071ecd4054_412040_1500x0_resize_catmullrom_3.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "If you extract all links from this tag using the `find_all` function, the results will contain same-page backlinks too, which you don't need. To avoid that, you need to select all `reference-text` span tags and extract the links from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.5334%2Fdsj-2021-024\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.5334%2Fdsj-2021-024\n",
      "/wiki/ISSN_(identifier)\n",
      "https://search.worldcat.org/issn/1683-1470\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:237719804\n",
      "http://www.searchenginehistory.com/\n",
      "https://web.archive.org/web/20161011080619/https://pdfs.semanticscholar.org/4fb4/3c5a212df751e84c3b2f8d29fabfe56c3616.pdf\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1145%2F1281192.1281287\n",
      "/wiki/ISBN_(identifier)\n",
      "/wiki/Special:BookSources/9781595936097\n",
      "/wiki/S2CID_(identifier)\n",
      "https://api.semanticscholar.org/CorpusID:833565\n",
      "https://pdfs.semanticscholar.org/4fb4/3c5a212df751e84c3b2f8d29fabfe56c3616.pdf\n",
      "http://www.gooseeker.com/en/node/knowledgebase/freeformat\n",
      "http://www.xconomy.com/san-francisco/2012/07/25/diffbot-is-using-computer-vision-to-reinvent-the-semantic-web/\n",
      "https://doi.org/10.1016%2Fj.jbusres.2020.06.012\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.1016%2Fj.jbusres.2020.06.012\n",
      "/wiki/ISSN_(identifier)\n",
      "https://search.worldcat.org/issn/0148-2963\n",
      "https://web.archive.org/web/20020308222536/http://www.chillingeffects.org/linking/faq.cgi#QID596\n",
      "http://www.chillingeffects.org/linking/faq.cgi#QID596\n",
      "http://scholarship.law.berkeley.edu/btlj/vol29/iss4/16/\n",
      "/wiki/Doi_(identifier)\n",
      "https://doi.org/10.15779%2FZ38B39B\n",
      "/wiki/ISSN_(identifier)\n",
      "https://search.worldcat.org/issn/1086-3818\n",
      "/wiki/Template:Cite_journal\n",
      "/wiki/Category:CS1_maint:_multiple_names:_authors_list\n",
      "http://www.tomwbell.com/NetLaw/Ch06.html\n",
      "https://web.archive.org/web/20020308222536/http://www.chillingeffects.org/linking/faq.cgi#QID460\n",
      "http://www.chillingeffects.org/linking/faq.cgi#QID460\n",
      "http://www.tomwbell.com/NetLaw/Ch07/Ticketmaster.html\n",
      "https://web.archive.org/web/20110723131832/http://www.fornova.net/documents/AAFareChase.pdf\n",
      "http://www.fornova.net/documents/AAFareChase.pdf\n",
      "https://web.archive.org/web/20160305025808/http://www.thefreelibrary.com/American+Airlines,+FareChase+Settle+Suit.-a0103213546\n",
      "http://www.thefreelibrary.com/American+Airlines,+FareChase+Settle+Suit.-a0103213546\n",
      "http://www.imperva.com/docs/WP_Detecting_and_Blocking_Site_Scraping_Attacks.pdf\n",
      "https://web.archive.org/web/20110211123854/http://library.findlaw.com/2003/Jul/29/132944.html\n",
      "http://library.findlaw.com/2003/Jul/29/132944.html\n",
      "https://web.archive.org/web/20130921054619/http://www.fornova.net/documents/Cvent.pdf\n",
      "http://www.fornova.net/documents/Cvent.pdf\n",
      "https://www.scribd.com/doc/249068700/LinkedIn-v-Resultly-LLC-Complaint?secret_password=pEVKDbnvhQL52oKfdrmT\n",
      "http://newmedialaw.proskauer.com/2014/12/05/qvc-sues-shopping-app-for-web-scraping-that-allegedly-triggered-site-outage/\n",
      "https://web.archive.org/web/20110723132015/http://www.fornova.net/documents/pblog-bna-com.pdf\n",
      "http://www.fornova.net/documents/pblog-bna-com.pdf\n",
      "https://www.techdirt.com/articles/20090605/2228205147.shtml\n",
      "https://www.eff.org/cases/facebook-v-power-ventures\n",
      "https://www.reuters.com/technology/us-supreme-court-revives-linkedin-bid-shield-personal-data-2021-06-14/\n",
      "/wiki/Reuters\n",
      "https://techcrunch.com/2022/04/18/web-scraping-legal-court/\n",
      "https://web.archive.org/web/20071012005033/http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf\n",
      "http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf\n",
      "http://www.bailii.org/ie/cases/IEHC/2010/H47.html\n",
      "https://web.archive.org/web/20120624103316/http://www.lkshields.ie/htmdocs/publications/newsletters/update26/update26_03.htm\n",
      "http://www.lkshields.ie/htmdocs/publications/newsletters/update26/update26_03.htm\n",
      "https://www.cnil.fr/fr/la-reutilisation-des-donnees-publiquement-accessibles-en-ligne-des-fins-de-demarchage-commercial\n",
      "https://medium.com/@finddatalab/can-you-still-perform-web-scraping-with-the-new-cnil-guidelines-bf3e20d0edc2\n",
      "https://web.archive.org/web/20191203113701/https://www.lloyds.com/~/media/5880dae185914b2487bed7bd63b96286.ashx\n",
      "https://www.lloyds.com/~/media/5880dae185914b2487bed7bd63b96286.ashx\n",
      "http://www.webstartdesign.com.au/spam_business_practical_guide.pdf\n",
      "https://proxyway.com/guides/what-is-web-scraping\n",
      "https://s3.us-west-2.amazonaws.com/research-papers-mynk/Breaking-Fraud-And-Bot-Detection-Solutions.pdf\n"
     ]
    }
   ],
   "source": [
    "# Select the **References section**\n",
    "references = browser.page.select_one('ol.references')\n",
    "\n",
    "# Select all span tags with the class `reference-text` to exclude backlinks\n",
    "references_list = references.select('span.reference-text')\n",
    "\n",
    "# For each reference span tag, find all anchor elements and print their href attribute values\n",
    "for reference in references_list:\n",
    "    link_tags = reference.find_all(\"a\")\n",
    "    \n",
    "    for link_tag in link_tags:\n",
    "        link = link_tag.get(\"href\")\n",
    "        if link:\n",
    "            print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Web Scraping with MechanicalSoup\n",
    "\n",
    "Now that you know how to scrape simple websites with `MechanicalSoup`, you can learn how to use it to scrape dynamic multipage websites. In this section, you'll scrape a sandbox called [Scrape This Site](https://www.scrapethissite.com/) . First, navigate to its home page:\n",
    "\n",
    "To navigate through web pages, you can use MechanicalSoup's `follow_link` function. You'll need to provide it with the target link that you want to navigate to, so locate and select the appropriate anchor tags on each page and extract their `href` attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.scrapethissite.com/\n"
     ]
    }
   ],
   "source": [
    "browser.open(\"https://www.scrapethissite.com/\")\n",
    "\n",
    "print(browser.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, find the link to the sandbox page from the navigation bar. If you inspect the page, you'll find that the link is inside an `<li>` tag that has the `id` as `nav-sandbox`. You can use the following code to extract and follow the link from this tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.scrapethissite.com/pages/\n"
     ]
    }
   ],
   "source": [
    "sandbox_nav_link = browser.page.select_one('li#nav-sandbox')\n",
    "\n",
    "sandbox_link = sandbox_nav_link.select_one('a')\n",
    "\n",
    "browser.follow_link(sandbox_link)\n",
    "\n",
    "print(browser.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, locate and follow the **Hockey Teams: Forms, Searching and Pagination** link. If you inspect the HTML structure around this element, you'll notice that each link on this page is inside `<div class=\"page\">` tags.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://www.scrapingbee.com/blog/getting-started-with-mechanicalsoup/inspect-sandbox-page_huff8e726660b72a3ae9da3d7a109ac6f7_854732_1500x0_resize_catmullrom_3.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract all these `divs` first, select the second `div`, and then select and extract the link from the `a` tag from within it. Here's the code you can use to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.scrapethissite.com/pages/forms/\n"
     ]
    }
   ],
   "source": [
    "sandbox_list_items = browser.page.select('div.page')\n",
    "\n",
    "hockey_list_item = sandbox_list_items[1]\n",
    "\n",
    "forms_sandbox_link = hockey_list_item.select_one('a')\n",
    "\n",
    "browser.follow_link(forms_sandbox_link)\n",
    "\n",
    "print(browser.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will bring your browser to the hockey teams information page. Putting all together, we can create a function to do the navigation for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate():\n",
    "    browser.open(\"https://www.scrapethissite.com/\")\n",
    "\n",
    "    print(browser.url)\n",
    "\n",
    "    sandbox_nav_link = browser.page.select_one('li#nav-sandbox')\n",
    "\n",
    "    sandbox_link = sandbox_nav_link.select_one('a')\n",
    "\n",
    "    browser.follow_link(sandbox_link)\n",
    "\n",
    "    print(browser.url)\n",
    "\n",
    "    sandbox_list_items = browser.page.select('div.page')\n",
    "\n",
    "    hockey_list_item = sandbox_list_items[1]\n",
    "\n",
    "    forms_sandbox_link = hockey_list_item.select_one('a')\n",
    "\n",
    "    browser.follow_link(forms_sandbox_link)\n",
    "\n",
    "    print(browser.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.scrapethissite.com/\n",
      "https://www.scrapethissite.com/pages/\n",
      "https://www.scrapethissite.com/pages/forms/\n"
     ]
    }
   ],
   "source": [
    "navigate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Forms\n",
    "\n",
    "Suppose that we want to extract information of teams that contain the word \"new\" in their name. To do this, we need to fill the search form with the word \"new\" and submit it. `MechanicalSoup` offers a `form` class to facilitate simpler form handling. You can select a form using its CSS selector and then easily pass input to it according to the names of its input fields.\n",
    "\n",
    "MechanicalSoup also provides a simple function called `submit_selected()` , which allows you to submit a form without having to locate and click its submit button.\n",
    "\n",
    "If you inspect the website's structure, you'll notice that the form element has the classes `form` and `form-inline`. You can use one of these to identify and select the form. Notice that the input box has been named `q`. You can use these two details to write the code for handling this page's search form in the `handle_form()` function. But first, let's write a function to extract the data from a Table.\n",
    "\n",
    "To extract the table data from the page, you'll use the `select_one` and `select` functions from Beautiful Soup, just as you did earlier in this tutorial. Save the following code in your `extract_table_data` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def extract_table_data(file_name):\n",
    "\n",
    "    # Select the table element\n",
    "    results = browser.page.select_one('table')\n",
    "\n",
    "    # Open a file and prepare a CSV Writer object\n",
    "    file = open(file_name, 'w')\n",
    "    csvwriter = csv.writer(file)\n",
    "\n",
    "    # Select the headers from the table first\n",
    "    headers = results.select('th')\n",
    "\n",
    "    # Create a temporary array to store the extract table header cells\n",
    "    temp_header_row = []\n",
    "\n",
    "    # For each header from the headers row, add it to the temporary array\n",
    "    for header in headers:\n",
    "        temp_header_row.append(header.string.strip().replace(\",\", \"\"))\n",
    "\n",
    "    # Write the temporary array to the CSV file\n",
    "    csvwriter.writerow(temp_header_row)\n",
    "    \n",
    "    # Next, select all rows of the table\n",
    "    rows = results.select('tr.team')\n",
    "\n",
    "    # For each row, prepare a temporary array containing all extracted cells and append it to the CSV file\n",
    "    for row in rows:\n",
    "        cells = row.select('td')\n",
    "        temp_row = []\n",
    "\n",
    "        for cell in cells:\n",
    "            temp_row.append(cell.string.strip().replace(\",\", \"\"))\n",
    "\n",
    "        csvwriter.writerow(temp_row)\n",
    "\n",
    "    # Close the CSV file at the end\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now goin back to the `handle_form` function, you can use the `extract_table_data` function to extract the data from the table after submitting the form. Here's the complete code for the `handle_form` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_form():\n",
    "    browser.select_form('form.form-inline')\n",
    "\n",
    "    browser.form.input({\"q\": \"new\"})\n",
    "\n",
    "    browser.submit_selected()\n",
    "\n",
    "    extract_table_data(\"first-page-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_form()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Pagination\n",
    "\n",
    "MechanicalSoup doesn't offer any special methods for handling paginations. You have to select the appropriate link from the pagination element and follow it via your browser object to navigate to the desired page.\n",
    "\n",
    "Here's what the HTML structure of the pagination element looks like:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://www.scrapingbee.com/blog/getting-started-with-mechanicalsoup/pagination-links-html-structure_hu2bf0938aecae8df0dd60999bb2ae106e_79616_825x0_resize_catmullrom_3.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, select the `<ul>` tag that has the pagination class added to it. Then, find the right `<li>` tag in it (based on the page number you want to navigate to), and finally, extract the `href` value from the `<a>` tag inside the `<li>` tag you identified. Here's the code to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_pagination():\n",
    "    pagination_links = browser.page.select_one('ul.pagination')\n",
    "\n",
    "    page_links = pagination_links.select('li')\n",
    "\n",
    "    # Choose the link at index 1 to navigate to page 2\n",
    "    next_page = page_links[1]\n",
    "\n",
    "    next_page_link = next_page.select_one('a')\n",
    "\n",
    "    browser.follow_link(next_page_link)\n",
    "\n",
    "    print(browser.url)\n",
    "\n",
    "    # Extract the table data from this page too\n",
    "    extract_table_data('second-page-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.scrapethissite.com/pages/forms/?page_num=2&q=new\n"
     ]
    }
   ],
   "source": [
    "handle_pagination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that two new CSV files will be created in the project directory: `first-page-results.csv` and `second-page-results.csv`. These will contain the results data extracted from the tables. This completes the tutorial for setting up web scraping using MechanicalSoup in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Thank you!<a class=\"tocSkip\"></center>\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
